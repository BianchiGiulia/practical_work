{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "n=5   | total interactions (user, item, label): 16021685  | unique users: 93971 | unique items: 55011 <br>\n",
    "n=15 | total interactions (user, item, label): 15797582  | unique users: 86338 | unique items: 26937\n",
    "\n",
    "\n",
    "[DeepFM](https://recbole.io/docs/user_guide/model/context/deepfm.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "from recbole.config import Config\n",
    "from recbole.data import create_dataset, data_preparation\n",
    "from recbole.model.context_aware_recommender import DeepFM\n",
    "from recbole.trainer import Trainer\n",
    "from recbole.utils import init_seed, init_logger\n",
    "import torch\n",
    "import logging\n",
    "from recbole.quick_start import  load_data_and_model\n",
    "\n",
    "#sanity check for mps\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[custom model doc](https://recbole.io/docs/developer_guide/customize_models.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_dict = {\n",
    "    'epochs': 10,\n",
    "    'data_path': '/Users/giulia/Desktop/tesi/',\n",
    "    'dataset': 'mind_small15',\n",
    "    #'load_col': {\n",
    "    #    'inter': ['user_id', 'item_id', 'label', 'timestamp'],\n",
    "    #    'useremb': ['uid', 'user_emb']\n",
    "    #},\n",
    "    'load_col': None, #doc: Note that if load_col is None, then all the existed atomic files will be loaded\n",
    "    'eval_args': {\n",
    "        'split': {'RS': [0.8, 0.1, 0.1]},\n",
    "        'group_by': 'user',\n",
    "        'order': 'RO',\n",
    "        'mode': 'labeled'},\n",
    "    'model': 'DeepFM',\n",
    "    'learning_rate': 0.001, \n",
    "    'device': device, #this doesn't work\n",
    "    'embedding_size': 32, # 64 -> kernel dies :(\n",
    "    'train_batch_size': 32, #64-> kernel dies :(\n",
    "    'eval_batch_size': 32,\n",
    "    'l2_reg': 0.001,\n",
    "    'early_stopping_patience': 5,  \n",
    "    'early_stopping_metric': 'AUC',\n",
    "    'checkpoint_dir': './saved',\n",
    "    'log_level': 'DEBUG',\n",
    "    'seed': 42,\n",
    "    'reproducibility': True,\n",
    "    'metrics': [\"AUC\", \"MAE\", \"RMSE\", \"LogLoss\"] #, \"MRR\", \"NDCG\", \"Precision\", \"Recall\", \"F1\" are not supported by DeepFM\n",
    "}\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[handler doc](https://docs.python.org/3/library/logging.handlers.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21 Apr 19:38    INFO  \n",
      "General Hyper Parameters:\n",
      "gpu_id = 0\n",
      "use_gpu = True\n",
      "seed = 42\n",
      "state = INFO\n",
      "reproducibility = True\n",
      "data_path = /Users/giulia/Desktop/tesi/mind_small15\n",
      "checkpoint_dir = ./saved\n",
      "show_progress = True\n",
      "save_dataset = False\n",
      "dataset_save_path = None\n",
      "save_dataloaders = False\n",
      "dataloaders_save_path = None\n",
      "log_wandb = False\n",
      "\n",
      "Training Hyper Parameters:\n",
      "epochs = 10\n",
      "train_batch_size = 32\n",
      "learner = adam\n",
      "learning_rate = 0.001\n",
      "train_neg_sample_args = {'distribution': 'none', 'sample_num': 'none', 'alpha': 'none', 'dynamic': False, 'candidate_num': 0}\n",
      "eval_step = 1\n",
      "stopping_step = 10\n",
      "clip_grad_norm = None\n",
      "weight_decay = 0.0\n",
      "loss_decimal_place = 4\n",
      "\n",
      "Evaluation Hyper Parameters:\n",
      "eval_args = {'split': {'RS': [0.8, 0.1, 0.1]}, 'order': 'RO', 'group_by': 'user', 'mode': {'valid': 'labeled', 'test': 'labeled'}}\n",
      "repeatable = False\n",
      "metrics = ['AUC', 'MAE', 'RMSE', 'LogLoss']\n",
      "topk = [10]\n",
      "valid_metric = AUC\n",
      "valid_metric_bigger = True\n",
      "eval_batch_size = 32\n",
      "metric_decimal_place = 4\n",
      "\n",
      "Dataset Hyper Parameters:\n",
      "field_separator = \t\n",
      "seq_separator =  \n",
      "USER_ID_FIELD = user_id\n",
      "ITEM_ID_FIELD = item_id\n",
      "RATING_FIELD = rating\n",
      "TIME_FIELD = timestamp\n",
      "seq_len = None\n",
      "LABEL_FIELD = label\n",
      "threshold = None\n",
      "NEG_PREFIX = neg_\n",
      "load_col = None\n",
      "unload_col = None\n",
      "unused_col = None\n",
      "additional_feat_suffix = None\n",
      "rm_dup_inter = None\n",
      "val_interval = None\n",
      "filter_inter_by_user_or_item = True\n",
      "user_inter_num_interval = [0,inf)\n",
      "item_inter_num_interval = [0,inf)\n",
      "alias_of_user_id = None\n",
      "alias_of_item_id = None\n",
      "alias_of_entity_id = None\n",
      "alias_of_relation_id = None\n",
      "preload_weight = None\n",
      "normalize_field = None\n",
      "normalize_all = None\n",
      "ITEM_LIST_LENGTH_FIELD = item_length\n",
      "LIST_SUFFIX = _list\n",
      "MAX_ITEM_LIST_LENGTH = 50\n",
      "POSITION_FIELD = position_id\n",
      "HEAD_ENTITY_ID_FIELD = head_id\n",
      "TAIL_ENTITY_ID_FIELD = tail_id\n",
      "RELATION_ID_FIELD = relation_id\n",
      "ENTITY_ID_FIELD = entity_id\n",
      "benchmark_filename = None\n",
      "\n",
      "Other Hyper Parameters: \n",
      "worker = 0\n",
      "wandb_project = recbole\n",
      "shuffle = True\n",
      "require_pow = False\n",
      "enable_amp = False\n",
      "enable_scaler = False\n",
      "transform = None\n",
      "embedding_size = 32\n",
      "mlp_hidden_size = [128, 128, 128]\n",
      "dropout_prob = 0.2\n",
      "numerical_features = []\n",
      "discretization = None\n",
      "kg_reverse_r = False\n",
      "entity_kg_num_interval = [0,inf)\n",
      "relation_kg_num_interval = [0,inf)\n",
      "MODEL_TYPE = ModelType.CONTEXT\n",
      "device = cpu\n",
      "l2_reg = 0.001\n",
      "early_stopping_patience = 5\n",
      "early_stopping_metric = AUC\n",
      "log_level = DEBUG\n",
      "MODEL_INPUT_TYPE = InputType.POINTWISE\n",
      "eval_type = EvaluatorType.VALUE\n",
      "single_spec = True\n",
      "local_rank = 0\n",
      "valid_neg_sample_args = {'distribution': 'none', 'sample_num': 'none'}\n",
      "test_neg_sample_args = {'distribution': 'none', 'sample_num': 'none'}\n",
      "\n",
      "\n",
      "\n",
      "General Hyper Parameters:\n",
      "gpu_id = 0\n",
      "use_gpu = True\n",
      "seed = 42\n",
      "state = INFO\n",
      "reproducibility = True\n",
      "data_path = /Users/giulia/Desktop/tesi/mind_small15\n",
      "checkpoint_dir = ./saved\n",
      "show_progress = True\n",
      "save_dataset = False\n",
      "dataset_save_path = None\n",
      "save_dataloaders = False\n",
      "dataloaders_save_path = None\n",
      "log_wandb = False\n",
      "\n",
      "Training Hyper Parameters:\n",
      "epochs = 10\n",
      "train_batch_size = 32\n",
      "learner = adam\n",
      "learning_rate = 0.001\n",
      "train_neg_sample_args = {'distribution': 'none', 'sample_num': 'none', 'alpha': 'none', 'dynamic': False, 'candidate_num': 0}\n",
      "eval_step = 1\n",
      "stopping_step = 10\n",
      "clip_grad_norm = None\n",
      "weight_decay = 0.0\n",
      "loss_decimal_place = 4\n",
      "\n",
      "Evaluation Hyper Parameters:\n",
      "eval_args = {'split': {'RS': [0.8, 0.1, 0.1]}, 'order': 'RO', 'group_by': 'user', 'mode': {'valid': 'labeled', 'test': 'labeled'}}\n",
      "repeatable = False\n",
      "metrics = ['AUC', 'MAE', 'RMSE', 'LogLoss']\n",
      "topk = [10]\n",
      "valid_metric = AUC\n",
      "valid_metric_bigger = True\n",
      "eval_batch_size = 32\n",
      "metric_decimal_place = 4\n",
      "\n",
      "Dataset Hyper Parameters:\n",
      "field_separator = \t\n",
      "seq_separator =  \n",
      "USER_ID_FIELD = user_id\n",
      "ITEM_ID_FIELD = item_id\n",
      "RATING_FIELD = rating\n",
      "TIME_FIELD = timestamp\n",
      "seq_len = None\n",
      "LABEL_FIELD = label\n",
      "threshold = None\n",
      "NEG_PREFIX = neg_\n",
      "load_col = None\n",
      "unload_col = None\n",
      "unused_col = None\n",
      "additional_feat_suffix = None\n",
      "rm_dup_inter = None\n",
      "val_interval = None\n",
      "filter_inter_by_user_or_item = True\n",
      "user_inter_num_interval = [0,inf)\n",
      "item_inter_num_interval = [0,inf)\n",
      "alias_of_user_id = None\n",
      "alias_of_item_id = None\n",
      "alias_of_entity_id = None\n",
      "alias_of_relation_id = None\n",
      "preload_weight = None\n",
      "normalize_field = None\n",
      "normalize_all = None\n",
      "ITEM_LIST_LENGTH_FIELD = item_length\n",
      "LIST_SUFFIX = _list\n",
      "MAX_ITEM_LIST_LENGTH = 50\n",
      "POSITION_FIELD = position_id\n",
      "HEAD_ENTITY_ID_FIELD = head_id\n",
      "TAIL_ENTITY_ID_FIELD = tail_id\n",
      "RELATION_ID_FIELD = relation_id\n",
      "ENTITY_ID_FIELD = entity_id\n",
      "benchmark_filename = None\n",
      "\n",
      "Other Hyper Parameters: \n",
      "worker = 0\n",
      "wandb_project = recbole\n",
      "shuffle = True\n",
      "require_pow = False\n",
      "enable_amp = False\n",
      "enable_scaler = False\n",
      "transform = None\n",
      "embedding_size = 32\n",
      "mlp_hidden_size = [128, 128, 128]\n",
      "dropout_prob = 0.2\n",
      "numerical_features = []\n",
      "discretization = None\n",
      "kg_reverse_r = False\n",
      "entity_kg_num_interval = [0,inf)\n",
      "relation_kg_num_interval = [0,inf)\n",
      "MODEL_TYPE = ModelType.CONTEXT\n",
      "device = cpu\n",
      "l2_reg = 0.001\n",
      "early_stopping_patience = 5\n",
      "early_stopping_metric = AUC\n",
      "log_level = DEBUG\n",
      "MODEL_INPUT_TYPE = InputType.POINTWISE\n",
      "eval_type = EvaluatorType.VALUE\n",
      "single_spec = True\n",
      "local_rank = 0\n",
      "valid_neg_sample_args = {'distribution': 'none', 'sample_num': 'none'}\n",
      "test_neg_sample_args = {'distribution': 'none', 'sample_num': 'none'}\n",
      "\n",
      "\n",
      "21 Apr 19:39    INFO  mind_small15\n",
      "The number of users: 86339\n",
      "Average actions of users: 182.97368482012556\n",
      "The number of items: 26938\n",
      "Average actions of items: 586.4640457363478\n",
      "The number of inters: 15797582\n",
      "The sparsity of the dataset: 99.320767816568%\n",
      "Remain Fields: ['user_id', 'item_id', 'label', 'timestamp']\n",
      "mind_small15\n",
      "The number of users: 86339\n",
      "Average actions of users: 182.97368482012556\n",
      "The number of items: 26938\n",
      "Average actions of items: 586.4640457363478\n",
      "The number of inters: 15797582\n",
      "The sparsity of the dataset: 99.320767816568%\n",
      "Remain Fields: ['user_id', 'item_id', 'label', 'timestamp']\n"
     ]
    }
   ],
   "source": [
    "config = Config(model='DeepFM', dataset=config_dict['dataset'], config_dict=config_dict)\n",
    "init_seed(config['seed'], config['reproducibility'])\n",
    "#------------logger\n",
    "init_logger(config)\n",
    "logger = logging.getLogger()\n",
    "#------------handler\n",
    "c_handler = logging.StreamHandler()\n",
    "c_handler.setLevel(logging.DEBUG)\n",
    "logger.addHandler(c_handler)\n",
    "logger.info(config)\n",
    "#------------data\n",
    "dataset = create_dataset(config)\n",
    "logger.info(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21 Apr 19:39    INFO  [Training]: train_batch_size = [32] train_neg_sample_args: [{'distribution': 'none', 'sample_num': 'none', 'alpha': 'none', 'dynamic': False, 'candidate_num': 0}]\n",
      "[Training]: train_batch_size = [32] train_neg_sample_args: [{'distribution': 'none', 'sample_num': 'none', 'alpha': 'none', 'dynamic': False, 'candidate_num': 0}]\n",
      "21 Apr 19:39    INFO  [Evaluation]: eval_batch_size = [32] eval_args: [{'split': {'RS': [0.8, 0.1, 0.1]}, 'order': 'RO', 'group_by': 'user', 'mode': {'valid': 'labeled', 'test': 'labeled'}}]\n",
      "[Evaluation]: eval_batch_size = [32] eval_args: [{'split': {'RS': [0.8, 0.1, 0.1]}, 'order': 'RO', 'group_by': 'user', 'mode': {'valid': 'labeled', 'test': 'labeled'}}]\n",
      "21 Apr 19:39    INFO  DeepFM(\n",
      "  (token_embedding_table): FMEmbedding(\n",
      "    (embedding): Embedding(113277, 32)\n",
      "  )\n",
      "  (first_order_linear): FMFirstOrderLinear(\n",
      "    (token_embedding_table): FMEmbedding(\n",
      "      (embedding): Embedding(113277, 1)\n",
      "    )\n",
      "  )\n",
      "  (fm): BaseFactorizationMachine()\n",
      "  (mlp_layers): MLPLayers(\n",
      "    (mlp_layers): Sequential(\n",
      "      (0): Dropout(p=0.2, inplace=False)\n",
      "      (1): Linear(in_features=64, out_features=128, bias=True)\n",
      "      (2): ReLU()\n",
      "      (3): Dropout(p=0.2, inplace=False)\n",
      "      (4): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (5): ReLU()\n",
      "      (6): Dropout(p=0.2, inplace=False)\n",
      "      (7): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (8): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (deep_predict_layer): Linear(in_features=128, out_features=1, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      "  (loss): BCEWithLogitsLoss()\n",
      ")\n",
      "Trainable parameters: 3779615\n",
      "DeepFM(\n",
      "  (token_embedding_table): FMEmbedding(\n",
      "    (embedding): Embedding(113277, 32)\n",
      "  )\n",
      "  (first_order_linear): FMFirstOrderLinear(\n",
      "    (token_embedding_table): FMEmbedding(\n",
      "      (embedding): Embedding(113277, 1)\n",
      "    )\n",
      "  )\n",
      "  (fm): BaseFactorizationMachine()\n",
      "  (mlp_layers): MLPLayers(\n",
      "    (mlp_layers): Sequential(\n",
      "      (0): Dropout(p=0.2, inplace=False)\n",
      "      (1): Linear(in_features=64, out_features=128, bias=True)\n",
      "      (2): ReLU()\n",
      "      (3): Dropout(p=0.2, inplace=False)\n",
      "      (4): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (5): ReLU()\n",
      "      (6): Dropout(p=0.2, inplace=False)\n",
      "      (7): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (8): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (deep_predict_layer): Linear(in_features=128, out_features=1, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      "  (loss): BCEWithLogitsLoss()\n",
      ")\n",
      "Trainable parameters: 3779615\n"
     ]
    }
   ],
   "source": [
    "train_data, valid_data, test_data = data_preparation(config, dataset)\n",
    "#------------model\n",
    "model = DeepFM(config, train_data.dataset).to(config['device'])\n",
    "logger.info(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21 Apr 20:56    INFO  \n",
      "General Hyper Parameters:\n",
      "gpu_id = 0\n",
      "use_gpu = True\n",
      "seed = 42\n",
      "state = INFO\n",
      "reproducibility = True\n",
      "data_path = /Users/giulia/Desktop/tesi/mind_small15\n",
      "checkpoint_dir = ./saved\n",
      "show_progress = True\n",
      "save_dataset = False\n",
      "dataset_save_path = None\n",
      "save_dataloaders = False\n",
      "dataloaders_save_path = None\n",
      "log_wandb = False\n",
      "\n",
      "Training Hyper Parameters:\n",
      "epochs = 10\n",
      "train_batch_size = 32\n",
      "learner = adam\n",
      "learning_rate = 0.001\n",
      "train_neg_sample_args = {'distribution': 'none', 'sample_num': 'none', 'alpha': 'none', 'dynamic': False, 'candidate_num': 0}\n",
      "eval_step = 1\n",
      "stopping_step = 10\n",
      "clip_grad_norm = None\n",
      "weight_decay = 0.0\n",
      "loss_decimal_place = 4\n",
      "\n",
      "Evaluation Hyper Parameters:\n",
      "eval_args = {'split': {'RS': [0.8, 0.1, 0.1]}, 'order': 'RO', 'group_by': 'user', 'mode': {'valid': 'labeled', 'test': 'labeled'}}\n",
      "repeatable = False\n",
      "metrics = ['AUC', 'MAE', 'RMSE', 'LogLoss']\n",
      "topk = [10]\n",
      "valid_metric = AUC\n",
      "valid_metric_bigger = True\n",
      "eval_batch_size = 32\n",
      "metric_decimal_place = 4\n",
      "\n",
      "Dataset Hyper Parameters:\n",
      "field_separator = \t\n",
      "seq_separator =  \n",
      "USER_ID_FIELD = user_id\n",
      "ITEM_ID_FIELD = item_id\n",
      "RATING_FIELD = rating\n",
      "TIME_FIELD = timestamp\n",
      "seq_len = None\n",
      "LABEL_FIELD = label\n",
      "threshold = None\n",
      "NEG_PREFIX = neg_\n",
      "load_col = None\n",
      "unload_col = None\n",
      "unused_col = None\n",
      "additional_feat_suffix = None\n",
      "rm_dup_inter = None\n",
      "val_interval = None\n",
      "filter_inter_by_user_or_item = True\n",
      "user_inter_num_interval = [0,inf)\n",
      "item_inter_num_interval = [0,inf)\n",
      "alias_of_user_id = None\n",
      "alias_of_item_id = None\n",
      "alias_of_entity_id = None\n",
      "alias_of_relation_id = None\n",
      "preload_weight = None\n",
      "normalize_field = None\n",
      "normalize_all = None\n",
      "ITEM_LIST_LENGTH_FIELD = item_length\n",
      "LIST_SUFFIX = _list\n",
      "MAX_ITEM_LIST_LENGTH = 50\n",
      "POSITION_FIELD = position_id\n",
      "HEAD_ENTITY_ID_FIELD = head_id\n",
      "TAIL_ENTITY_ID_FIELD = tail_id\n",
      "RELATION_ID_FIELD = relation_id\n",
      "ENTITY_ID_FIELD = entity_id\n",
      "benchmark_filename = None\n",
      "\n",
      "Other Hyper Parameters: \n",
      "worker = 0\n",
      "wandb_project = recbole\n",
      "shuffle = True\n",
      "require_pow = False\n",
      "enable_amp = False\n",
      "enable_scaler = False\n",
      "transform = None\n",
      "embedding_size = 32\n",
      "mlp_hidden_size = [128, 128, 128]\n",
      "dropout_prob = 0.2\n",
      "numerical_features = []\n",
      "discretization = None\n",
      "kg_reverse_r = False\n",
      "entity_kg_num_interval = [0,inf)\n",
      "relation_kg_num_interval = [0,inf)\n",
      "MODEL_TYPE = ModelType.CONTEXT\n",
      "device = cpu\n",
      "l2_reg = 0.001\n",
      "early_stopping_patience = 5\n",
      "early_stopping_metric = AUC\n",
      "log_level = DEBUG\n",
      "MODEL_INPUT_TYPE = InputType.POINTWISE\n",
      "eval_type = EvaluatorType.VALUE\n",
      "single_spec = True\n",
      "local_rank = 0\n",
      "valid_neg_sample_args = {'distribution': 'none', 'sample_num': 'none'}\n",
      "test_neg_sample_args = {'distribution': 'none', 'sample_num': 'none'}\n",
      "\n",
      "\n",
      "\n",
      "General Hyper Parameters:\n",
      "gpu_id = 0\n",
      "use_gpu = True\n",
      "seed = 42\n",
      "state = INFO\n",
      "reproducibility = True\n",
      "data_path = /Users/giulia/Desktop/tesi/mind_small15\n",
      "checkpoint_dir = ./saved\n",
      "show_progress = True\n",
      "save_dataset = False\n",
      "dataset_save_path = None\n",
      "save_dataloaders = False\n",
      "dataloaders_save_path = None\n",
      "log_wandb = False\n",
      "\n",
      "Training Hyper Parameters:\n",
      "epochs = 10\n",
      "train_batch_size = 32\n",
      "learner = adam\n",
      "learning_rate = 0.001\n",
      "train_neg_sample_args = {'distribution': 'none', 'sample_num': 'none', 'alpha': 'none', 'dynamic': False, 'candidate_num': 0}\n",
      "eval_step = 1\n",
      "stopping_step = 10\n",
      "clip_grad_norm = None\n",
      "weight_decay = 0.0\n",
      "loss_decimal_place = 4\n",
      "\n",
      "Evaluation Hyper Parameters:\n",
      "eval_args = {'split': {'RS': [0.8, 0.1, 0.1]}, 'order': 'RO', 'group_by': 'user', 'mode': {'valid': 'labeled', 'test': 'labeled'}}\n",
      "repeatable = False\n",
      "metrics = ['AUC', 'MAE', 'RMSE', 'LogLoss']\n",
      "topk = [10]\n",
      "valid_metric = AUC\n",
      "valid_metric_bigger = True\n",
      "eval_batch_size = 32\n",
      "metric_decimal_place = 4\n",
      "\n",
      "Dataset Hyper Parameters:\n",
      "field_separator = \t\n",
      "seq_separator =  \n",
      "USER_ID_FIELD = user_id\n",
      "ITEM_ID_FIELD = item_id\n",
      "RATING_FIELD = rating\n",
      "TIME_FIELD = timestamp\n",
      "seq_len = None\n",
      "LABEL_FIELD = label\n",
      "threshold = None\n",
      "NEG_PREFIX = neg_\n",
      "load_col = None\n",
      "unload_col = None\n",
      "unused_col = None\n",
      "additional_feat_suffix = None\n",
      "rm_dup_inter = None\n",
      "val_interval = None\n",
      "filter_inter_by_user_or_item = True\n",
      "user_inter_num_interval = [0,inf)\n",
      "item_inter_num_interval = [0,inf)\n",
      "alias_of_user_id = None\n",
      "alias_of_item_id = None\n",
      "alias_of_entity_id = None\n",
      "alias_of_relation_id = None\n",
      "preload_weight = None\n",
      "normalize_field = None\n",
      "normalize_all = None\n",
      "ITEM_LIST_LENGTH_FIELD = item_length\n",
      "LIST_SUFFIX = _list\n",
      "MAX_ITEM_LIST_LENGTH = 50\n",
      "POSITION_FIELD = position_id\n",
      "HEAD_ENTITY_ID_FIELD = head_id\n",
      "TAIL_ENTITY_ID_FIELD = tail_id\n",
      "RELATION_ID_FIELD = relation_id\n",
      "ENTITY_ID_FIELD = entity_id\n",
      "benchmark_filename = None\n",
      "\n",
      "Other Hyper Parameters: \n",
      "worker = 0\n",
      "wandb_project = recbole\n",
      "shuffle = True\n",
      "require_pow = False\n",
      "enable_amp = False\n",
      "enable_scaler = False\n",
      "transform = None\n",
      "embedding_size = 32\n",
      "mlp_hidden_size = [128, 128, 128]\n",
      "dropout_prob = 0.2\n",
      "numerical_features = []\n",
      "discretization = None\n",
      "kg_reverse_r = False\n",
      "entity_kg_num_interval = [0,inf)\n",
      "relation_kg_num_interval = [0,inf)\n",
      "MODEL_TYPE = ModelType.CONTEXT\n",
      "device = cpu\n",
      "l2_reg = 0.001\n",
      "early_stopping_patience = 5\n",
      "early_stopping_metric = AUC\n",
      "log_level = DEBUG\n",
      "MODEL_INPUT_TYPE = InputType.POINTWISE\n",
      "eval_type = EvaluatorType.VALUE\n",
      "single_spec = True\n",
      "local_rank = 0\n",
      "valid_neg_sample_args = {'distribution': 'none', 'sample_num': 'none'}\n",
      "test_neg_sample_args = {'distribution': 'none', 'sample_num': 'none'}\n",
      "\n",
      "\n",
      "21 Apr 20:57    INFO  mind_small15\n",
      "The number of users: 86339\n",
      "Average actions of users: 182.97368482012556\n",
      "The number of items: 26938\n",
      "Average actions of items: 586.4640457363478\n",
      "The number of inters: 15797582\n",
      "The sparsity of the dataset: 99.320767816568%\n",
      "Remain Fields: ['user_id', 'item_id', 'label', 'timestamp']\n",
      "mind_small15\n",
      "The number of users: 86339\n",
      "Average actions of users: 182.97368482012556\n",
      "The number of items: 26938\n",
      "Average actions of items: 586.4640457363478\n",
      "The number of inters: 15797582\n",
      "The sparsity of the dataset: 99.320767816568%\n",
      "Remain Fields: ['user_id', 'item_id', 'label', 'timestamp']\n",
      "21 Apr 20:58    INFO  [Training]: train_batch_size = [32] train_neg_sample_args: [{'distribution': 'none', 'sample_num': 'none', 'alpha': 'none', 'dynamic': False, 'candidate_num': 0}]\n",
      "[Training]: train_batch_size = [32] train_neg_sample_args: [{'distribution': 'none', 'sample_num': 'none', 'alpha': 'none', 'dynamic': False, 'candidate_num': 0}]\n",
      "21 Apr 20:58    INFO  [Evaluation]: eval_batch_size = [32] eval_args: [{'split': {'RS': [0.8, 0.1, 0.1]}, 'order': 'RO', 'group_by': 'user', 'mode': {'valid': 'labeled', 'test': 'labeled'}}]\n",
      "[Evaluation]: eval_batch_size = [32] eval_args: [{'split': {'RS': [0.8, 0.1, 0.1]}, 'order': 'RO', 'group_by': 'user', 'mode': {'valid': 'labeled', 'test': 'labeled'}}]\n"
     ]
    }
   ],
   "source": [
    "#LOAD MODEL\n",
    "checkpoint_path = './saved/DeepFM-Apr-21-2024_16-02-34.pth' #spec at the end of respective log file\n",
    "\n",
    "config, model, dataset, train_data, valid_data, test_data = load_data_and_model(\n",
    "    model_file=checkpoint_path,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(config, model)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21 Apr 21:59    INFO  epoch 0 training [time: 3697.44s, train loss: 72543.7085]\n",
      "epoch 0 training [time: 3697.44s, train loss: 72543.7085]\n",
      "21 Apr 22:00    INFO  epoch 0 evaluating [time: 35.19s, valid_score: 0.985100]\n",
      "epoch 0 evaluating [time: 35.19s, valid_score: 0.985100]\n",
      "21 Apr 22:00    INFO  valid result: \n",
      "auc : 0.9851    mae : 0.0338    rmse : 0.166    logloss : 0.3216\n",
      "valid result: \n",
      "auc : 0.9851    mae : 0.0338    rmse : 0.166    logloss : 0.3216\n",
      "21 Apr 22:00    INFO  Saving current: ./saved/DeepFM-Apr-21-2024_20-58-12.pth\n",
      "Saving current: ./saved/DeepFM-Apr-21-2024_20-58-12.pth\n",
      "21 Apr 23:01    INFO  epoch 1 training [time: 3672.91s, train loss: 84947.7648]\n",
      "epoch 1 training [time: 3672.91s, train loss: 84947.7648]\n",
      "21 Apr 23:02    INFO  epoch 1 evaluating [time: 36.01s, valid_score: 0.984800]\n",
      "epoch 1 evaluating [time: 36.01s, valid_score: 0.984800]\n",
      "21 Apr 23:02    INFO  valid result: \n",
      "auc : 0.9848    mae : 0.0337    rmse : 0.1697    logloss : 0.3823\n",
      "valid result: \n",
      "auc : 0.9848    mae : 0.0337    rmse : 0.1697    logloss : 0.3823\n",
      "22 Apr 00:04    INFO  epoch 2 training [time: 3751.02s, train loss: 98132.7527]\n",
      "epoch 2 training [time: 3751.02s, train loss: 98132.7527]\n",
      "22 Apr 00:05    INFO  epoch 2 evaluating [time: 36.27s, valid_score: 0.984000]\n",
      "epoch 2 evaluating [time: 36.27s, valid_score: 0.984000]\n",
      "22 Apr 00:05    INFO  valid result: \n",
      "auc : 0.984    mae : 0.0336    rmse : 0.1754    logloss : 0.4925\n",
      "valid result: \n",
      "auc : 0.984    mae : 0.0336    rmse : 0.1754    logloss : 0.4925\n",
      "22 Apr 01:07    INFO  epoch 3 training [time: 3727.07s, train loss: 110368.7987]\n",
      "epoch 3 training [time: 3727.07s, train loss: 110368.7987]\n",
      "22 Apr 01:08    INFO  epoch 3 evaluating [time: 37.10s, valid_score: 0.983300]\n",
      "epoch 3 evaluating [time: 37.10s, valid_score: 0.983300]\n",
      "22 Apr 01:08    INFO  valid result: \n",
      "auc : 0.9833    mae : 0.0365    rmse : 0.1839    logloss : 0.5735\n",
      "valid result: \n",
      "auc : 0.9833    mae : 0.0365    rmse : 0.1839    logloss : 0.5735\n",
      "22 Apr 02:09    INFO  epoch 4 training [time: 3701.54s, train loss: 121551.8062]\n",
      "epoch 4 training [time: 3701.54s, train loss: 121551.8062]\n",
      "22 Apr 02:10    INFO  epoch 4 evaluating [time: 36.40s, valid_score: 0.982400]\n",
      "epoch 4 evaluating [time: 36.40s, valid_score: 0.982400]\n",
      "22 Apr 02:10    INFO  valid result: \n",
      "auc : 0.9824    mae : 0.0376    rmse : 0.1882    logloss : 0.6652\n",
      "valid result: \n",
      "auc : 0.9824    mae : 0.0376    rmse : 0.1882    logloss : 0.6652\n",
      "22 Apr 03:12    INFO  epoch 5 training [time: 3698.48s, train loss: 130885.9389]\n",
      "epoch 5 training [time: 3698.48s, train loss: 130885.9389]\n",
      "22 Apr 03:12    INFO  epoch 5 evaluating [time: 36.73s, valid_score: 0.981800]\n",
      "epoch 5 evaluating [time: 36.73s, valid_score: 0.981800]\n",
      "22 Apr 03:12    INFO  valid result: \n",
      "auc : 0.9818    mae : 0.0376    rmse : 0.1898    logloss : 0.7627\n",
      "valid result: \n",
      "auc : 0.9818    mae : 0.0376    rmse : 0.1898    logloss : 0.7627\n",
      "22 Apr 04:13    INFO  epoch 6 training [time: 3670.53s, train loss: 138356.7120]\n",
      "epoch 6 training [time: 3670.53s, train loss: 138356.7120]\n",
      "22 Apr 04:14    INFO  epoch 6 evaluating [time: 36.82s, valid_score: 0.980700]\n",
      "epoch 6 evaluating [time: 36.82s, valid_score: 0.980700]\n",
      "22 Apr 04:14    INFO  valid result: \n",
      "auc : 0.9807    mae : 0.0389    rmse : 0.1941    logloss : 0.8756\n",
      "valid result: \n",
      "auc : 0.9807    mae : 0.0389    rmse : 0.1941    logloss : 0.8756\n",
      "22 Apr 05:15    INFO  epoch 7 training [time: 3666.70s, train loss: 145069.2395]\n",
      "epoch 7 training [time: 3666.70s, train loss: 145069.2395]\n",
      "22 Apr 05:16    INFO  epoch 7 evaluating [time: 36.52s, valid_score: 0.980200]\n",
      "epoch 7 evaluating [time: 36.52s, valid_score: 0.980200]\n",
      "22 Apr 05:16    INFO  valid result: \n",
      "auc : 0.9802    mae : 0.0385    rmse : 0.1938    logloss : 0.9491\n",
      "valid result: \n",
      "auc : 0.9802    mae : 0.0385    rmse : 0.1938    logloss : 0.9491\n",
      "22 Apr 06:17    INFO  epoch 8 training [time: 3667.95s, train loss: 153020.1742]\n",
      "epoch 8 training [time: 3667.95s, train loss: 153020.1742]\n",
      "22 Apr 06:17    INFO  epoch 8 evaluating [time: 37.07s, valid_score: 0.979300]\n",
      "epoch 8 evaluating [time: 37.07s, valid_score: 0.979300]\n",
      "22 Apr 06:17    INFO  valid result: \n",
      "auc : 0.9793    mae : 0.0405    rmse : 0.1984    logloss : 0.9882\n",
      "valid result: \n",
      "auc : 0.9793    mae : 0.0405    rmse : 0.1984    logloss : 0.9882\n",
      "22 Apr 07:19    INFO  epoch 9 training [time: 3701.26s, train loss: 160205.4209]\n",
      "epoch 9 training [time: 3701.26s, train loss: 160205.4209]\n",
      "22 Apr 07:20    INFO  epoch 9 evaluating [time: 37.29s, valid_score: 0.979300]\n",
      "epoch 9 evaluating [time: 37.29s, valid_score: 0.979300]\n",
      "22 Apr 07:20    INFO  valid result: \n",
      "auc : 0.9793    mae : 0.039    rmse : 0.1952    logloss : 1.036\n",
      "valid result: \n",
      "auc : 0.9793    mae : 0.039    rmse : 0.1952    logloss : 1.036\n"
     ]
    }
   ],
   "source": [
    "#training should show log info now\n",
    "best_valid_score, best_valid_result = trainer.fit(train_data, valid_data, saved=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22 Apr 07:20    INFO  Loading model structure and parameters from ./saved/DeepFM-Apr-21-2024_20-58-12.pth\n",
      "Loading model structure and parameters from ./saved/DeepFM-Apr-21-2024_20-58-12.pth\n",
      "22 Apr 07:20    INFO  OrderedDict([('auc', 0.985), ('mae', 0.034), ('rmse', 0.1666), ('logloss', 0.3239)])\n",
      "OrderedDict([('auc', 0.985), ('mae', 0.034), ('rmse', 0.1666), ('logloss', 0.3239)])\n"
     ]
    }
   ],
   "source": [
    "test_result = trainer.evaluate(test_data)\n",
    "logger.info(test_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "my issue/question\n",
    "- <s>[SPLIT DATA ISSUE](https://chat.openai.com/share/699f3143-fe80-4a2a-8e75-eb5392212b80) ---> [Modify/extend sampler?](https://recbole.io/docs/developer_guide/customize_samplers.html) // in alternativa unire train+val e lasciare che Recbole faccia lo split--> [SEE THIS](https://recbole.io/docs/recbole/recbole.quick_start.quick_start.html#recbole.quick_start.quick_start.load_data_and_model) -> </s> [(LRS) to use pre-configured train\\validation\\test data splits](https://github.com/RUCAIBox/RecBole/pull/1950): doesn't work :(\n",
    "- [value-based metrics and ranking-based metrics can not be used together](https://recbole.io/docs/user_guide/train_eval_intro.html) -> 2 run x model?\n",
    "\n",
    "- Can't seem to make it run on mps, only cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"config_dict = {\\n    'data_path': './mind',\\n    'dataset': 'mind',\\n    'eval_args': {\\n    'split': {'LRS': None},\\n    'order': 'RO'  ,# not relevant\\n    'group_by': '-', # not relevant\\n    'mode': 'full'  # Train, validation, test split\\n}}\\n\\nconfig = Config(model='DMF', dataset=config_dict['dataset'], config_dict=config_dict)\\ndataset = create_dataset(config)\\n\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#LRS\n",
    "\"\"\"config_dict = {\n",
    "    'data_path': './mind',\n",
    "    'dataset': 'mind',\n",
    "    'eval_args': {\n",
    "    'split': {'LRS': None},\n",
    "    'order': 'RO'  ,# not relevant\n",
    "    'group_by': '-', # not relevant\n",
    "    'mode': 'full'  # Train, validation, test split\n",
    "}}\n",
    "\n",
    "config = Config(model='DMF', dataset=config_dict['dataset'], config_dict=config_dict)\n",
    "dataset = create_dataset(config)\n",
    "\"\"\"\n",
    "#"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------------------------------------------------------------\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "just personal notes & useful links:<br>\n",
    "[ADD-> TF-IDF EMBEDDING](https://recbole.io/docs/user_guide/usage/load_pretrained_embedding.html)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[kaggle example to get prediction](https://www.kaggle.com/code/astrung/recbole-lstm-sequential-for-recomendation-tutorial#4.-Create-recommendation-result-from-trained-model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "[MODELS link](https://recbole.io/docs/user_guide/model_intro.html)\n",
    "\n",
    "higher FLOPS->higher complexity & more computation\n",
    "\n",
    "\n",
    "\n",
    "save_dataset (bool): Determines whether the processed dataset is saved to disk. This can be useful for large datasets that take a long time to preprocess, as it allows for quicker loading in subsequent runs.\n",
    "\n",
    "[training hyperparam.](https://recbole.io/docs/user_guide/config/training_settings.html)\n",
    "\n",
    "[clip_grad_norm](https://pytorch.org/docs/stable/generated/torch.nn.utils.clip_grad_norm_.html): This parameter is used to prevent the exploding gradient problem by clipping the gradients of the parameters during backpropagation to have a maximum norm of a specified value. If set to None, no clipping is applied. If you specify a number (e.g., 5.0), it will clip the gradients such that their norm does not exceed this value. Gradient clipping can be crucial for stabilizing the training of **deep learning models**.\n",
    "\n",
    "[eval hyperparam](https://recbole.io/docs/user_guide/config/evaluation_settings.html)\n",
    "reproducibility vs repeatible (args):\n",
    "\n",
    "**Reproducibility**: set to True, the framework will explicitly set random seeds for all underlying libraries (e.g., PyTorch, NumPy) and any internal operations that use random numbers. This ensures that every aspect of the computation, from the way data is split to the initialization of model parameters, is consistent across runs.\n",
    "\n",
    "**Repeatable**: False might allow for variability in how data is sampled, ordered, or split during the evaluation phase, potentially leading to slight differences in evaluation metrics across runs. Conversely, setting it to True would fix these aspects to ensure consistency in evaluation outcomes.\n",
    "\n",
    "\n",
    "[data hyp](https://recbole.io/docs/user_guide/config/data_settings.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "recbole-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
